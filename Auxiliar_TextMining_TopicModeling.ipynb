{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "El modelamiento de tópicos, es una herramienta estadística que busca encontrar los temas presentes en un conjunto de documentos (corpus), permitiendo organizar, buscar, indexar, explorar y comprender grandes colecciones de documentos.\\\\\n",
    "\n",
    "\n",
    "En este sentido, los temas se pueden definir como ``un patrón repetitivo de términos co-currentes en un corpus''. Por ejemplo, se tiene el siguiente tópico, representado por sus cuatro palabras más probables, ``salud'', ``médico'', ``paciente'', ``hospital'', estas palabras sugieren el siguiente nombre para el tema: ``Atención médica‘’.En los modelos convencionales de tópicos:\n",
    "1. Las palabras dentro de un documento y los documentos son tratados como intercambiables.\n",
    "2. La mayoría de estos fijan el número de tópicos y lo  mantienen así a lo largo del corpus completo.\n",
    "\n",
    "\n",
    "\n",
    "### VISUALIZACIÓN DE QUE ES UN TÓPICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import itertools\n",
    "import re\n",
    "from numpy.linalg import norm\n",
    "from time import time\n",
    "\n",
    "#Preprocesamiento\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS #importar set de stopwords\n",
    "from nltk.stem import SnowballStemmer #importar stemmer\n",
    "nlp = spacy.load('es_core_news_sm') #python -m spacy download es\n",
    "\n",
    "#Bag-of-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Topic modeling\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base de datos\n",
    "La base de datos consiste de 5 conjuntos de noticias extraídas de la radio biobio. \n",
    "Cada conjunto de noticias contiene 200 documentos (noticias) y tiene asociado una categoría en {nacional, internacional, economía, sociedad, opinion}.\n",
    "\n",
    "**Creditos**: \n",
    "\n",
    "- Pablo Badilla Torrealba \n",
    "- https://github.com/dccuchile/CC6205/tree/master/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_biobio.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc, sep=None, stopwords_remove =False, lemmatization=False, stemming = False, vocabulary=None):\n",
    "    '''\n",
    "    Por defecto divide la sentencia por el carácter espacio.\n",
    "    Ej: 'Data Mining is the best course'->['Data',  'Mining', 'is', 'the', 'best', 'course']\n",
    "    \n",
    "    Input: \n",
    "    1. doc: str, documento.\n",
    "    2. sep: str, carácter para dividir el documento en tokens, por defecto es el espacio.\n",
    "    3. stopwords_remove: bool, si es True remueve los stopwords del documento.\n",
    "    4. lemmatization: bool, si es True lleva las palabras a su lema.\n",
    "    5. stemming: bool, si es True lleva las palabas a su raíz.\n",
    "    6. vocabuary: list, si un vocabulario es dado filtra las palabras que no estan presentes en el.\n",
    "    \n",
    "    Output: \n",
    "    list, lista de tokens.\n",
    "    \n",
    "    Nota: aplicar stemming y lemmatization al mismo tiempo no es correcto.\n",
    "    '''\n",
    "    doc = re.sub(r'[^\\w\\s]','', doc) #elimina los símbolos de puntuación\n",
    "    doc = re.sub(r'[a-zA-Z]+[0-9]+', '', doc) #elimina los caracteres que contienen letras y números\n",
    "    doc = re.sub(r'[0-9]+', ' ', doc) #elimina los caracteres numéricos\n",
    "   \n",
    "    tokens = doc.split(sep) #tokenización\n",
    "    tokens = [word.lower() for word in tokens] #pasar todas las palabras a minúsculas\n",
    "    \n",
    "    \n",
    "    \n",
    "    if stopwords_remove ==True: #remover stopwords y palabras con menos de tres caracteres\n",
    "        tokens = [word for word in tokens if word not in STOP_WORDS and len(word)>2]\n",
    "    \n",
    "    if lemmatization==True:\n",
    "        tokens = [nlp(word)[0].lemma_ for word in tokens]\n",
    "        \n",
    "    if stemming == True:\n",
    "        stemmer = SnowballStemmer('spanish')\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    if vocabulary is not None:\n",
    "        tokens = [word for word in tokens if word in vocabulary]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latent Dirichlet Allocation\n",
    " \n",
    "Sean  *K* tópicos, $\\beta_{1:K}$ son distribuciones de probabilidad sobre un vocabulario fijo, dibujadas por una $Dirichlet(\\eta)$. Para cada documento $d$ del corpus $D$ se asume que es dibujado por el siguiente proceso generativo:\n",
    "\n",
    "\n",
    "1. Escoger la mezcla de tópicos $\\theta_{d}$ de una distribución sobre un $(K− 1)-simplex$, tal como una $Dirichlet(\\alpha)$.\n",
    "2. Para cada palabra:\n",
    "    - Escoger la asignación del tópico $z\\sim Mult(\\theta_{d})$\n",
    "    - Escoger una palabra $w \\sim Mult(\\beta_{z})$\n",
    "\n",
    "\n",
    "<img src='img/lda_graphical_model.png'>\n",
    "\n",
    " \n",
    "### Ejemplo de output\n",
    "### Revisar Murphy Mixture Models\n",
    "### K-means\n",
    " \n",
    "Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el diccionario a partir de los textos procesados en el formato que necesita LDA en gensim\n",
    "dictionary = Dictionary(newcorpus)\n",
    "\n",
    "#Creamos el corpus para darle al modelo (segun el formato de esta libreria)\n",
    "#El corpus contiene una representacion numerica de los textos, un texto es representada por una lista de tuplas\n",
    "#donde el primer elemento de la tupla es la id de la palabra y el segundo es su frecuencia de aparición en el texto.\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in newcorpus]\n",
    "\n",
    "#guardamos el diccionario y el corpus\n",
    "dictionary.save('dictionary.dict')\n",
    "MmCorpus.serialize('corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Primer elemento del diccionario o bolsa de palabras:', dictionary[0])\n",
    "print('Representación del corpus en el formato que requiere la librería: \\n', corpus[0:2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda_k_5=LdaModel(corpus=corpus, id2word=dictionary, num_topics=5) \n",
    "Lda_k_5.save('Lda_k_5.model') # guardamos el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización LDA\n",
    "\n",
    "Las visualizaciones en modelamiento de tópicos nos ayudan a responder tres preguntas:\n",
    "\n",
    "1. **¿Cuál es el significado de cada tópico?**\n",
    "2. **¿Cuán predominante es cada tópico?**\n",
    "3. **¿Cómo se relacionan los tópicos entre sí?**\n",
    "\n",
    "\n",
    "\n",
    "Sievert, C., & Shirley, K. (2014), desarrollaron una herramienta de visualización para responder estas preguntas. La herramienta a través de una visualización espacial responde la pregunta 2 y 3.  \n",
    "\n",
    "Además para responder la pregunta 1 incorporan un gráfico de barras a la derecha del gráfico espacial que muestra las palabras más relevantes del tópico seleccionado dado un parámetro $\\lambda$ entre 0 y 1,  entonces, la relevancia de la palabra w en el tópico $k$ dado $\\lambda$ esta dada a  través de la siguiente formula:\n",
    "\\begin{equation*}\n",
    "r(w,k|\\lambda) = \\lambda log(\\phi_{k,w})+(1-\\lambda)log(\\frac{\\phi_{k,w}}{p_{w}}), \\lambda \\in [0,1]$$\n",
    "\\end{equation*}\n",
    "Donde $\\phi_{k,w}$ es la probabilidad de que el término $w$ sea generado por el tópico $k$, $p_{w}$ es la probabilidad de el término $w$ en el corpus.\\\\  \n",
    "\n",
    "\n",
    "\\footnote{Sievert, C., & Shirley, K. (2014). LDAvis: A method for visualizing and interpreting topics. In Proceedings of the workshop on interactive language learning, visualization, and interfaces (pp. 63-70).} \n",
    "\n",
    "\\href{http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN\\%20Topic\\%20Model\\%20Talk.ipynb#topic=3&lambda=0.46&term=}{ \\texttt{Click aquí: LDAvis example}}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparar los datos para generarar la visualización de LDA\n",
    "lda_display = pyLDAvis.gensim.prepare(Lda_k_5, corpus, dictionary, sort_topics=True, R=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
